# ML Track-Week 8

Now, Let us boost our learning.  
This week will introduce you to algorithms that can boost the acuuracy of your model.  
Boosting considers many models, placed sequentially where each model tries to minimize the error obtained from the previous model.  

We'll learn About  

- Gradient Boosting Algorithm  
- Extreme Boosting Algorithm  
- Ada Boost Algorithm  



### Click [here](https://github.com/kabirnagpal/SoA-ML-14/blob/master/week%208.ipynb) to view the Jupyter-Notebook.   
If you don't have any Python Environment, you can also try the code in [Google Colab](https://colab.research.google.com/).  


Question to be answered after you complete your notebook
Which of the following algorithm are not an example of ensemble learning algorithm?

A. Random Forest
B. Adaboost
C. Extra Trees
D. Gradient Boosting
E. Decision Trees

<form method='POST'>
  <input name='answer'>
  <input type='submit' value='Submit'>
  <code class='code_checker'>
  def answer(s):
      return s.lower() =='e':
             
  </code>
</form>
