<a href='https://t.me/ml_code_for_100_days'><button>Discuss on telegram</button></a>
# ML Track-Week 8

Now, let us boost our learning.  
This week will introduce you to algorithms that can boost the accuracy of your model.  
Boosting considers many models, placed sequentially where each model tries to minimize the error obtained from the previous model.  

We'll learn About  

1. Gradient Boosting Algorithm  
2. Extreme Boosting Algorithm  
3. Ada Boost Algorithm  

**Click [here](https://github.com/kabirnagpal/SoA-ML-14/blob/master/week%208.ipynb) to view the Jupyter-Notebook.**   
If you don't have any Python Environment, you can also try the code in [Google Colab](https://colab.research.google.com/).  


We hope you've gone through the code and other resources provided along. Let's wind it up with a quick question.  
Which of the following algorithm are not an example of ensemble learning algorithm?  

1. Random Forest
2. Adaboost
3. Extra Trees
4. Gradient Boosting
5. Decision Trees

<form method='POST'>
  <input name='answer'>
  <input type='submit' value='Submit'>
</form>
